# -*- coding: utf-8 -*-
"""techne_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eyGM-wfwhswAhE3ESum_7XVFd0_2142Y
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.metrics import mean_squared_error, mean_absolute_error

df = pd.read_csv('hv_new.csv')

df.drop(['Gender','Age'], axis=1, inplace=True)

df.to_csv('hv_updated.csv', index=False)

df = pd.read_csv('hv_updated.csv')
# Add timestamp
start_date = '2021-01-01'
df['Timestamp'] = pd.date_range(start=start_date, periods=len(df), freq='W')

# Optionally, set the timestamp as the index for time series analysis
df.set_index('Timestamp', inplace=True)

class_intervals = [0, 5000, 8000, float('inf')]
class_labels = [0, 1, 2]

# Map the fitness score values to class labels based on intervals
df['Fitness Class'] = pd.cut(df['Fitness Score'], bins=class_intervals, labels=class_labels)

# Display the DataFrame with fitness classes
print(df)

df.drop(['Fitness Score'], axis=1, inplace=True)
df.to_csv('hv_week.csv', index=True)

def load_data(file_path):
    df = pd.read_csv(file_path)
    df['Timestamp'] = pd.to_datetime(df['Timestamp'])
    df.set_index('Timestamp', inplace=True)
    return df

def prepare_data(data, sequence_length):
    X = data.iloc[:, :-1]
    y = data.iloc[:, -1]
    return X, y

def main():
    sequence_length = 10
    data = load_data("hv_days.csv")  # Specify the path to your CSV file
    X, y = prepare_data(data, sequence_length)

    # Scale the features
    scaler_X = MinMaxScaler()
    X_scaled = scaler_X.fit_transform(X)

    # Scale the target variable
    scaler_y = MinMaxScaler()
    y = y.values.reshape(-1, 1)  # Reshape y to be a 2D array
    y_scaled = scaler_y.fit_transform(y)

    # Split data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=28)

    # Create TimeseriesGenerators for training and validation
    train_generator = TimeseriesGenerator(X_train, y_train, length=sequence_length, batch_size=1)
    validation_generator = TimeseriesGenerator(X_test, y_test, length=sequence_length, batch_size=1)

    # Define LSTM model
    model = Sequential()
    model.add(LSTM(50, activation='relu', input_shape=(sequence_length, X_train.shape[1])))
    model.add(Dense(1))
    model.compile(optimizer='adadelta', loss='mse')

    # Train the model
    model.fit(train_generator, validation_data=validation_generator, epochs=10)

    # Predict using the validation generator
    y_pred = []
    for i in range(len(validation_generator)):
        x_val, y_val = validation_generator[i]
        y_pred.extend(model.predict(x_val))

    # Inverse transform the scaled predictions to original values
    y_pred_original = scaler_y.inverse_transform(np.array(y_pred).reshape(-1, 1)).flatten()

    # # Inverse transform the scaled y_test to original values
    # y_test_original = scaler_y.inverse_transform(y_test).flatten()
    # Extract y_test_original from validation_generator for consistent lengths
    y_test_original = []
    for i in range(len(validation_generator)):
        _, y_val = validation_generator[i]
        y_test_original.extend(scaler_y.inverse_transform(y_val))
    # Calculate performance metrics
    mse = mean_squared_error(y_test_original, y_pred_original)
    mae = mean_absolute_error(y_test_original, y_pred_original)
    rmse = np.sqrt(mse)

    print("Mean Squared Error:", mse)
    print("Mean Absolute Error:", mae)
    print("Root Mean Squared Error:", rmse)

if __name__ == "__main__":
    main()

def load_data(file_path):
    df = pd.read_csv(file_path)
    df['Timestamp'] = pd.to_datetime(df['Timestamp'])
    df.set_index('Timestamp', inplace=True)
    return df

def prepare_data(data, sequence_length):
    X = data.iloc[:, :-1]
    y = data.iloc[:, -1]
    return X, y

def main():
    sequence_length = 10
    data = load_data("hv_hrs.csv")  # Specify the path to your CSV file
    X, y = prepare_data(data, sequence_length)

    # Scale the features
    scaler_X = MinMaxScaler()
    X_scaled = scaler_X.fit_transform(X)

    # Scale the target variable
    scaler_y = MinMaxScaler()
    y = y.values.reshape(-1, 1)  # Reshape y to be a 2D array
    y_scaled = scaler_y.fit_transform(y)

    # Split data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.3, random_state=32)

    # Create TimeseriesGenerators for training and validation
    train_generator = TimeseriesGenerator(X_train, y_train, length=sequence_length, batch_size=1)
    validation_generator = TimeseriesGenerator(X_test, y_test, length=sequence_length, batch_size=1)

    # Define LSTM model
    model = Sequential()
    model.add(LSTM(50, activation='relu', input_shape=(sequence_length, X_train.shape[1])))
    model.add(Dense(1))
    model.compile(optimizer='adadelta', loss='mse')

    # Train the model
    model.fit(train_generator, validation_data=validation_generator, epochs=10)

    # Predict using the validation generator
    y_pred = []
    for i in range(len(validation_generator)):
        x_val, y_val = validation_generator[i]
        y_pred.extend(model.predict(x_val))

    # Inverse transform the scaled predictions to original values
    y_pred_original = scaler_y.inverse_transform(np.array(y_pred).reshape(-1, 1)).flatten()

    # # Inverse transform the scaled y_test to original values
    # y_test_original = scaler_y.inverse_transform(y_test).flatten()
    # Extract y_test_original from validation_generator for consistent lengths
    y_test_original = []
    for i in range(len(validation_generator)):
        _, y_val = validation_generator[i]
        y_test_original.extend(scaler_y.inverse_transform(y_val))
    # Calculate performance metrics
    mse = mean_squared_error(y_test_original, y_pred_original)
    mae = mean_absolute_error(y_test_original, y_pred_original)
    rmse = np.sqrt(mse)

    print("Mean Squared Error:", mse)
    print("Mean Absolute Error:", mae)
    print("Root Mean Squared Error:", rmse)

if __name__ == "__main__":
    main()



def load_data(file_path):
    df = pd.read_csv(file_path)
    df['Timestamp'] = pd.to_datetime(df['Timestamp'])
    df.set_index('Timestamp', inplace=True)
    return df

def prepare_data(data, sequence_length):
    X = data.iloc[:, :-1]
    y = data.iloc[:, -1]
    return X, y

def main():
    sequence_length = 10
    data = load_data("hv_week.csv")  # Specify the path to your CSV file
    X, y = prepare_data(data, sequence_length)

    # Scale the features
    scaler_X = MinMaxScaler()
    X_scaled = scaler_X.fit_transform(X)

    # Scale the target variable
    scaler_y = MinMaxScaler()
    y = y.values.reshape(-1, 1)  # Reshape y to be a 2D array
    y_scaled = scaler_y.fit_transform(y)

    # Split data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=32)

    # Create TimeseriesGenerators for training and validation
    train_generator = TimeseriesGenerator(X_train, y_train, length=sequence_length, batch_size=1)
    validation_generator = TimeseriesGenerator(X_test, y_test, length=sequence_length, batch_size=1)

    # Define LSTM model
    model = Sequential()
    model.add(LSTM(50, activation='relu', input_shape=(sequence_length, X_train.shape[1])))
    model.add(Dense(1))
    model.compile(optimizer='adadelta', loss='mse')

    # Train the model
    model.fit(train_generator, validation_data=validation_generator, epochs=10)

    # Predict using the validation generator
    y_pred = []
    for i in range(len(validation_generator)):
        x_val, y_val = validation_generator[i]
        y_pred.extend(model.predict(x_val))

    # Inverse transform the scaled predictions to original values
    y_pred_original = scaler_y.inverse_transform(np.array(y_pred).reshape(-1, 1)).flatten()

    # # Inverse transform the scaled y_test to original values
    # y_test_original = scaler_y.inverse_transform(y_test).flatten()
    # Extract y_test_original from validation_generator for consistent lengths
    y_test_original = []
    for i in range(len(validation_generator)):
        _, y_val = validation_generator[i]
        y_test_original.extend(scaler_y.inverse_transform(y_val))
    # Calculate performance metrics
    mse = mean_squared_error(y_test_original, y_pred_original)
    mae = mean_absolute_error(y_test_original, y_pred_original)
    rmse = np.sqrt(mse)

    print("Mean Squared Error:", mse)
    print("Mean Absolute Error:", mae)
    print("Root Mean Squared Error:", rmse)

if __name__ == "__main__":
    main()









