# -*- coding: utf-8 -*-
"""feature_importance .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ojCkkIzZtEaqUEGF3qjtuGF1ThldWnT2
"""

df0 = pd.read_csv('original_data.csv')

df0.drop(['Person ID', 'Occupation','Quality of Sleep','Physical Activity Level','BMI Category','Sleep Disorder'], axis=1, inplace=True)

# Loaing dataset in dataframe 'df0'
df0 = pd.read_csv('original_data.csv')

# Assuming 'Gender' is a categorical variable that needs encoding
df0['Gender'] = df0['Gender'].map({'Male': 0, 'Female': 1})

# Preprocess the 'Blood Pressure' column
df0[['Systolic', 'Diastolic']] = df0['Blood Pressure'].str.split('/', expand=True)
df0[['Systolic', 'Diastolic']] = df0[['Systolic', 'Diastolic']].astype(float)

# Drop the original 'Blood Pressure' column
df0.drop(columns=['Blood Pressure'], inplace=True)


weights = {
    'Gender': 0.05,
    'Age': 0.1,
    'Sleep Duration': 0.2,
    'Stress Level': 0.15,
    'Heart Rate': 0.15,
    'Daily Steps': 0.1,
    'Systolic': 0.15,
    'Diastolic': 0.1
}

col= ['Gender','Age', 'Sleep Duration', 'Stress Level', 'Heart Rate', 'Daily Steps', 'Systolic', 'Diastolic']

# Calculate the fitness score using the defined weights
df0['Fitness Score'] = df0[col].dot(pd.Series(weights))

# Save the updated dataset with the fitness score
df0.to_csv('hv.csv', index=False)

# Load the dataset
df = pd.read_csv('hv.csv')

# Separate features and target variable
X = df.drop(columns=['Fitness Score'])
y = df['Fitness Score']


# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=28)

# Normalize the features
scaler_X = MinMaxScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)

# Normalize the target
scaler_y = MinMaxScaler()
y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))

# Build the Neural Network model
model = Sequential()
model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='linear'))


# Compile the model
optimizer = Nadam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='mse')


# Train the model
history = model.fit(X_train_scaled, y_train_scaled, epochs=180, batch_size=32, validation_split=0.2)


# Make predictions
X_test_scaled = scaler_X.transform(X_test)
y_pred_scaled = model.predict(X_test_scaled).flatten()

# Inverse transform to get values in the original range
y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()


# Calculate errors
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mse)

print(f'Mean Squared Error: {mse}')
print(f'R-squared (R2) Score: {r2}')
print(f'Mean Absolute Error (MAE): {mae}')
print(f'Root Mean Squared Error (RMSE): {rmse}')


# Save the trained model to an HDF5 file
model.save('techne_ANN.h5')

# Save the scalers to pickle files
with open('scaler_X.pkl', 'wb') as f:
    pickle.dump(scaler_X, f)

with open('scaler_y.pkl', 'wb') as f:
    pickle.dump(scaler_y, f)

"""#feature importances"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
# from sklearn.metrics import mean_squared_error, r2_score
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
df1 = pd.read_csv('hv.csv')

# Normalize the health vitals data (optional)
scaler = MinMaxScaler()
columns_to_normalize = ['Heart Rate', 'Stress Level', 'Systolic', 'Diastolic', 'Sleep Duration']
df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])

# Split the data into features (X) and target (y)
X = df1.drop(columns=['Fitness Score'])
y = df1['Fitness Score']

# Split the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=28)

# Create the Random Forest Regressor model
rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model on the training data
rf_regressor.fit(X_train, y_train)

# Predict the fitness score on the test set
y_pred = rf_regressor.predict(X_test)

# Evaluate the model's performance
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred, squared=False)

print(f'Mean Squared Error: {mse}')
print(f'R-squared (R2) Score: {r2}')
print(f'Mean Absolute Error (MAE) {mae}')
print(f'Root Mean Squared Error (RMSE): {rmse}')

from sklearn.ensemble import RandomForestRegressor

# Initialize the RandomForestRegressor with desired parameters
rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model on the training data
rf_regressor.fit(X_train, y_train)

# Get feature importances
feature_importances = rf_regressor.feature_importances_

# Pair the feature names with their corresponding importance scores
feature_names = X_train.columns
feature_importance_map = dict(zip(feature_names, feature_importances))

# Calculate feature weights based on the importance scores
total_importance = sum(feature_importances)
feature_weights = {feature: importance / total_importance for feature, importance in feature_importance_map.items()}

# Print the feature weights in descending order
sorted_feature_weights = dict(sorted(feature_weights.items(), key=lambda item: item[1], reverse=True))
for feature, weight in sorted_feature_weights.items():
    print(f"{feature}: {weight}")

